###Intro to Cypress

All the information you ever needed, including setting up an account, can be found here
https://wiki.hpc.tulane.edu/trac/wiki/cypress#CodingonCypress

* Please make sure you have reviwed the cypress wiki before Thursday and in particular, how to submit job requests (https://wiki.hpc.tulane.edu/trac/wiki/cypress/using#SubmittingJobsonCypress)

Once your account is set up, logging in is easy. Replace "eenbody" with your Tulane username and then type your Tulane password when prompted.

```bash
ssh eenbody@cypress1.tulane.edu
```

###Useful unix commands
```bash
cd .. 	#goes back to the previous directory
mv 	#use this command to rename a file, you must include the name of the file followed by the new name
rm –r [folder] 	#to delete folders with contents, MUST GIVE A FOLDER, be careful! Can delete important folders, like root!
zcat 		#takes a compressed file to decompress .gz and run through cat
grep 	#searches gnu regular expressions, will have to use ^ to find something at beginning of line and $ to find at the end of the line – very powerful – you can use this to find all the reads that didn’t work 
zgrep   #can use this to also look up things – for example look up a barcode in a fastq file 
control z #then type# bg 	#use this when you want to push a command into the back ground – it will eventually pop out a number when the command is finished 
```

###pyRAD Tutorial on cypress

Now we can run the pyRAD tutorial on the cluster (http://nbviewer.jupyter.org/gist/dereneaton/1f661bfb205b644086cc/tutorial_RAD_3.0.ipynb)

* Open a new terminal window (not on cypress) and download the pyRAD RAD tutorial data into your current directory

```bash
wget -q dereneaton.com/downloads/simRADs.zip
unzip simRADs.zip
```

* Next, we're going to copy the folder over to your directory on cypress. Just replace eenbody with your username. 
* You can also transfer the zipped file if unzipping it did not create a new directory, and then unzip that file in your cypress directory.

```bash
scp ./simRADs/ eenbody@cypress1.tulane.edu:/home/eenbody
```

* Either log into cypress or return to the window where you had cypress open. 
* Navigate into the simRADs directory. 
* Create a new job ticket file

```bash
nano pyrad_n.srun
```

* The contents should be this (make sure you understand what each line means, via the wiki above). pyRAD is already installed as a module on cypress and to run it you must load it in (module load pyrad) and execute it using pyrad (lowercase). 

```bash

#!/bin/bash
#SBATCH --job-name=OneHourJob ### Job Name
#SBATCH --nodes=1             ### Node count required for the job
#SBATCH --ntasks-per-node=1   ### Nuber of tasks to be launched per Node

module load pyrad

pyrad -n
```

* Execute the command using sbatch

```bash
sbatch pyrad_n.srun
```

* Just like running this on your desktop, if this ran successfully, you should have a params.txt file in your directory.

* On cypress, you should have no issues editing the params.txt file using sed

```bash
%%bash
sed -i '/## 7. /c\2                   ## 7. N processors... ' params.txt
sed -i '/## 10. /c\.85                ## 10. lowered clust thresh... ' params.txt
sed -i '/## 14. /c\c85m4p3            ## 14. outprefix... ' params.txt
sed -i '/## 24./c\8                   ## 24. maxH raised ... ' params.txt
sed -i '/## 30./c\*                   ## 30. all output formats... ' params.txt
```

* Now, create a new job request file called pyrad1.srun with the following info, then run it with sbatch. You should be able to do the same with all 7 steps of the RAD tutorial. 

```bash
#!/bin/bash
#SBATCH --job-name=OneHourJob ### Job Name
#SBATCH --nodes=1             ### Node count required for the job
#SBATCH --ntasks-per-node=1   ### Nuber of tasks to be launched per Node

module load pyrad

pyrad -p params.txt -s 1
```

####Running pyRAD with a real dataset

#####*How to copy data to the cluster*

We are going to use pyRAD to call SNPs on Sara L's dataset from her work with jacana. These are data generated by a slightly different method than that above, as her samples were sequenced by genoytpe by sequencing (GBS) methods. As far as pyRAD is concerned, things won't change that much. 

* First, you will need to copy the raw data to your lab's project folder. Only one member per lab should do this. You will also need to physically go to the derryberry lab to initiate this transfer. I will send instructions by email about the password.

* Once you are at the computer, open terminal and login to cypress. You want to find your way to the lustre project folder for your lab and make a directory for jacanas. For example for the karubian lab see below. You should only have permission to access your workgroup's folder.

```bash
cd /lustre/project/jk
mkdir pyrad-zdiazmar
cd pyrad-zdiazmar
```

* Now that we are here, lets transfer those files. They are large files and total ~60gb, so using the "scp" command is a bit unwieldy as it was built for smaller files. Instead we will use bcbp. You will want to specify the login information of your local computer. The following example is for the Blum lab login.

```bash
bbcp -zv -r "blumlab@eeb-globus.tulane.edu:/Volumes/LaCie/Jacana_GBS_Raw_Reads" ./
```

-v stands for verbose 
-r is for recursive (copies all files in the directory)
-z this is used in this case to transfer files from another ip (i.e. from the mac tower), but you wouldn't need it if you were sitting at the tower instead I believe..

You should get some output like this (each line comes in slowly)

```bash
bbcp: Indexing files to be copied...
bbcp: Copying 5 files and 0 links in 1 directory.
File Jacana_Raw_Reads/Jacana GBS Raw Reads/.DS_Store created; 6148 bytes at 255.8 KB/s
File Jacana_Raw_Reads/Jacana GBS Raw Reads/C6RL0ANXX_1_fastq created; 67982686805 bytes at 76.8 MB/s
File Jacana_Raw_Reads/Jacana GBS Raw Reads/C6RL0ANXX_1_fastq.gz created; 18058122029 bytes at 67.9 MB/s
File Jacana_Raw_Reads/Jacana GBS Raw Reads/C6RL0ANXX_2_fastq.gz created; 17324891025 bytes at 67.2 MB/s
File Jacana_Raw_Reads/Jacana GBS Raw Reads/C6RL0ANXX_3_fastq.gz created; 17453590465 bytes at 64.3 MB/s
5 files copied at effectively 71.7 MB/s\
```

* I'm not sure how long mine took, but at least half an hour I think. Now you are ready to use cypress to dig through this dataset!

Now, we're ready to run pyrad. 

#####*Running pyrad with a full dataset*

* First, we need to create a new params file

```bash
module load pyrad
pyrad -n
```

* You should see a new file, params.txt. We need to edit this params file to read our fastq file for plate 1 and provide it with a barcodes file. In our directory, there are three fastq files that correspond to the three plates of DNA that Sara ran. The barcodes overlap in these three plates, so we will have to run step 1 (demultiplexing) three times. The change to line two reflects this, where we only want it to consider the first plate. Line three specifies the one barcode file we are using (jacana1 corresponds to plate 1). 

* Here is what I have in my params file by copying the commands used in the pyRAD GBS tutorial.

```bash
==** parameter inputs for pyRAD version 3.0.66  **======================== affected step ==
./                        ## 1. Working directory                                 (all)
./C6RL0ANXX_1.fastq.gz    ## 2. Loc. of non-demultiplexed files (if not line 18)  (s1)
./*.barcodes              ## 3. Loc. of barcode file (if not line 18)             (s1)
vsearch                   ## 4. command (or path) to call vsearch (or usearch)    (s3,s6)
muscle                    ## 5. command (or path) to call muscle                  (s3,s7)
TGCAG                     ## 6. Restriction overhang (e.g., C|TGCAG -> TGCAG)     (s1,s2)
20                         ## 7. N processors (parallel)                           (all)
6                         ## 8. Mindepth: min coverage for a cluster              (s4,s5)
4                         ## 9. NQual: max # sites with qual < 20 (or see line 20)(s2)
.85                 ## 10. lowered clust thresh...
gbs                 ## 11. changed datatype to gbs
4                         ## 12. MinCov: min samples in a final locus             (s7)
3                         ## 13. MaxSH: max inds with shared hetero site          (s7)
c85m4p3             ## 14. outprefix...
==== optional params below this line ===================================  affected step ==
                       ## 15.opt.: select subset (prefix* only selector)            (s2-s7)
                       ## 16.opt.: add-on (outgroup) taxa (list or prefix*)         (s6,s7)
                       ## 17.opt.: exclude taxa (list or prefix*)                   (s7)
                       ## 18.opt.: loc. of de-multiplexed data                      (s2)
                       ## 19.opt.: maxM: N mismatches in barcodes (def= 1)          (s1)
                       ## 20.opt.: phred Qscore offset (def= 33)                    (s2)
1                    ## 21. set filter to 1
                       ## 22.opt.: a priori E,H (def= 0.001,0.01, if not estimated) (s5)
                       ## 23.opt.: maxN: max Ns in a cons seq (def=5)               (s5)
8                    ## 24. increased maxH
                       ## 25.opt.: ploidy: max alleles in cons seq (def=2;see docs) (s4,s5)
                       ## 26.opt.: maxSNPs: (def=100). Paired (def=100,100)         (s7)
                       ## 27.opt.: maxIndels: within-clust,across-clust (def. 3,99) (s3,s7)
                       ## 28.opt.: random number seed (def. 112233)              (s3,s6,s7)
                       ## 29.opt.: trim overhang left,right on final loci, def(0,0) (s7)
*                    ## 30. all output formats
                       ## 31.opt.: maj. base call at depth>x<mindepth (def.x=mindepth) (s5)
50                   ## 32. keep fragments longer than 50
                       ## 33.opt.: max stack size (int), def= max(500,mean+2*SD)    (s3)
                       ## 34.opt.: minDerep: exclude dereps with <= N copies, def=1 (s3)
                       ## 35.opt.: use hierarchical clustering (def.=0, 1=yes)      (s6)
                       ## 36.opt.: repeat masking (def.=1='dust' method, 0=no)      (s3,s6)
                       ## 37.opt.: vsearch max threads per job (def.=6; see docs)   (s3,s6)
==== optional: list group/clade assignments below this line (see docs) ==================
```

* Here is Sara's params file that I believe everyone was using during the workshop on Thursday. Either is probably fine. 

```bash
==** parameter inputs for pyRAD version 3.0.66  **======================== affected step ==
./                        ## 1. Working directory                                 (all)
./C6RL0ANXX_1_fastq.gz              ## 2. Loc. of non-demultiplexed files (if not line 18)  (s1)
./jacana1.barcodes              ## 3. Loc. of barcode file (if not line 18)             (s1)
vsearch                   ## 4. command (or path) to call vsearch (or usearch)    (s3,s6)
muscle                    ## 5. command (or path) to call muscle                  (s3,s7)
TGCAG                     ## 6. Restriction overhang (e.g., C|TGCAG -> TGCAG)     (s1,s2)
20                         ## 7. N processors (parallel)                           (all)
6                         ## 8. Mindepth: min coverage for a cluster              (s4,s5)
4                         ## 9. NQual: max # sites with qual < 20 (or see line 20)(s2)
.88                       ## 10. Wclust: clustering threshold as a decimal        (s3,s6)
rad                       ## 11. Datatype: rad,gbs,pairgbs,pairddrad,(others:see docs)(all)
4                         ## 12. MinCov: min samples in a final locus             (s7)
3                         ## 13. MaxSH: max inds with shared hetero site          (s7)
c88d6m4p3                 ## 14. Prefix name for final output (no spaces)         (s7)
```


* Now, because we are on the cluster, we need to create a job script to run step one. I've called my script pyrad_1.srun.

```bash
#!/bin/bash
#SBATCH --qos=normal
#SBATCH --job-name=sara_pyrad_s1.1 ### Job Name
#SBATCH --nodes=1             ### Node count required for the job
#SBATCH --ntasks-per-node=20   ### Nuber of tasks to be launched per Node
#SBATCH --output=jacanas1.1output.out
#SBATCH --error=jacana1error.err

module load pyrad

pyrad -p params.txt -s 1
```

* Ok so this hasn't actually worked yet, we're note sure why! Might try changing qos=long

* One issue could have been the barcodes file, since it was copied from excel and still has tabs instead of spaces. Let's replace tabs and then rename the files with the following commands:

```bash
cat jacana1.barcodes | sed -e 's/['$'\011'' ]\+/ /' | tee jacana1new.barcodes
mv jacana1.barcodes jacana1old.barcodes
mv jacana1new.barcodes jacana1.barcodes
```

* We also modified our script so that we can run step 1 on each plates without overwriting the output stats and fastq folders: 

```bash
#!/bin/bash
#SBATCH --qos=normal
#SBATCH --time=1-0
#SBATCH --verbose    ###        Verbosity logs error information into the error file
#SBATCH --job-name=jacana_pyrad_s1.1 ### Job Name
#SBATCH --nodes=1             ### Node count required for the job
#SBATCH --ntasks-per-node=20   ### Number of tasks to be launched per Node
#SBATCH --output=jacanas.1.1.output.out
#SBATCH --error=jacanas.1.1.error.err
#SBATCH --mail-type=ALL
#SBATCH --mail-user=slipshut@tulane.edu

date
pwd

mkdir w$SLURM_JOBID
cp params1.1.txt w$SLURM_JOBID/params.txt
ln C6RL0ANXX_1_fastq.gz w$SLURM_JOBID
ln jacana1.barcodes w$SLURM_JOBID
cd w$SLURM_JOBID


module load pyrad

pyrad -p params.txt -s 1

date (END) 
```

* Continue running step 1 for plates 2 and 3 by making new params.txt files and running new scripts or editing the first one

* We were able to successfully run step 1 of pyrad using the new script and new params.txt files. After this finished we explored the outputs.
```bash
ls fastq/ | wc –l 	#tells you how many files you have in this fastq folder
wc -l jacana1.barcodes 	#tells you how many lines are in this file (95 is ok, 96 is ok too – depends on how computer is counting lines)
```

* Once a job is submitted you will have a new folder for that job – you want to keep the right folder, rename it (using the mv command) and get rid of old ones
```bash
ls w[folder number] 	#this should show all the inputs and outputs from this job including a stats and fastq

du --si w[folder number] 	#this is another way to see if it worked, it shows you what’s in the folder and how much space it takes up
```

* We renamed our successful job output folders and went into them to explore the fastq and stats outputs

* We saw that line 19 (maxM)in the params file sets levels of more or les stringency – the default is 1 (one mismatch), but you can change this

```bash
cat stats/s1.sorting.txt | column –t | head 	#This will show you the beginning of the file with number of reads, etc in columns
```


