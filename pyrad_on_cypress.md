###Intro to Cypress

All the information you ever needed, including setting up an account, can be found here
https://wiki.hpc.tulane.edu/trac/wiki/cypress#CodingonCypress

* Please make sure you have reviwed the cypress wiki before Thursday and in particular, how to submit job requests (https://wiki.hpc.tulane.edu/trac/wiki/cypress/using#SubmittingJobsonCypress)

Once your account is set up, logging in is easy. Replace "eenbody" with your Tulane username and then type your Tulane password when prompted.

```bash
ssh eenbody@cypress1.tulane.edu
```

###Useful unix commands
```bash
cd .. 	#goes back to the previous directory
mv 	#use this command to rename a file, you must include the name of the file followed by the new name
rm –r [folder] 	#to delete folders with contents, MUST GIVE A FOLDER, be careful! Can delete important folders, like root!
zcat 		#takes a compressed file to decompress .gz and run through cat
grep 	#searches gnu regular expressions, will have to use ^ to find something at beginning of line and $ to find at the end of the line – very powerful – you can use this to find all the reads that didn’t work 
zgrep   #can use this to also look up things – for example look up a barcode in a fastq file 
control z #then type# bg 	#use this when you want to push a command into the back ground – it will eventually pop out a number when the command is finished 
```

###pyRAD Tutorial on cypress

Now we can run the pyRAD tutorial on the cluster (http://nbviewer.jupyter.org/gist/dereneaton/1f661bfb205b644086cc/tutorial_RAD_3.0.ipynb)

* Open a new terminal window (not on cypress) and download the pyRAD RAD tutorial data into your current directory

```bash
wget -q dereneaton.com/downloads/simRADs.zip
unzip simRADs.zip
```

* Next, we're going to copy the folder over to your directory on cypress. Just replace eenbody with your username. 
* You can also transfer the zipped file if unzipping it did not create a new directory, and then unzip that file in your cypress directory.

```bash
scp ./simRADs/ eenbody@cypress1.tulane.edu:/home/eenbody
```

* Either log into cypress or return to the window where you had cypress open. 
* Navigate into the simRADs directory. 
* Create a new job ticket file

```bash
nano pyrad_n.srun
```

* The contents should be this (make sure you understand what each line means, via the wiki above). pyRAD is already installed as a module on cypress and to run it you must load it in (module load pyrad) and execute it using pyrad (lowercase). 

```bash

#!/bin/bash
#SBATCH --job-name=OneHourJob ### Job Name
#SBATCH --nodes=1             ### Node count required for the job
#SBATCH --ntasks-per-node=1   ### Nuber of tasks to be launched per Node

module load pyrad

pyrad -n
```

* Execute the command using sbatch

```bash
sbatch pyrad_n.srun
```

* Just like running this on your desktop, if this ran successfully, you should have a params.txt file in your directory.

* On cypress, you should have no issues editing the params.txt file using sed

```bash
%%bash
sed -i '/## 7. /c\2                   ## 7. N processors... ' params.txt
sed -i '/## 10. /c\.85                ## 10. lowered clust thresh... ' params.txt
sed -i '/## 14. /c\c85m4p3            ## 14. outprefix... ' params.txt
sed -i '/## 24./c\8                   ## 24. maxH raised ... ' params.txt
sed -i '/## 30./c\*                   ## 30. all output formats... ' params.txt
```

* Now, create a new job request file called pyrad1.srun with the following info, then run it with sbatch. You should be able to do the same with all 7 steps of the RAD tutorial. 

```bash
#!/bin/bash
#SBATCH --job-name=OneHourJob ### Job Name
#SBATCH --nodes=1             ### Node count required for the job
#SBATCH --ntasks-per-node=1   ### Nuber of tasks to be launched per Node

module load pyrad

pyrad -p params.txt -s 1
```

###Running pyRAD with a real dataset

####*How to copy data to the cluster*

We are going to use pyRAD to call SNPs on Sara L's dataset from her work with jacana. These are data generated by a slightly different method than that above, as her samples were sequenced by genoytpe by sequencing (GBS) methods. As far as pyRAD is concerned, things won't change that much. 

* First, you will need to copy the raw data to your lab's project folder. Only one member per lab should do this and we will copy files to cypress from our personal computers, via the shared lab mac tower (you will need your lab's login for this computer). 

* Open terminal and login to cypress. You want to find your way to the lustre project folder for your lab and make a directory for jacanas. For example for the karubian lab see below. You should only have permission to access your workgroup's folder.

```bash
cd /lustre/project/jk
mkdir pyrad-zdiazmar
cd pyrad-zdiazmar
```

* Now that we are here, lets transfer those files. They are large files and total ~60gb, so using the "scp" command is a bit unwieldy as it was built for smaller files. Instead we will use bcbp. You will need to specify the login information of your local computer. The following example is for the Karubian lab login. This will copy the entire folder called Jacana_GBS_Raw_Reads (the raw illumina data) to the current directory (in this case pyrad-zdiazmar). 

```bash
bbcp -zv -r "karubianlab@eeb-globus.tulane.edu:/Volumes/LaCie/Jacana_GBS_Raw_Reads" ./
```

-v stands for verbose 
-r is for recursive (copies all files in the directory)
-z this is used in this case to transfer files from another ip (i.e. from the mac tower), but you wouldn't need it if you were sitting at the tower instead I believe..

You should get some output like this (each line comes in slowly)

```bash
bbcp: Indexing files to be copied...
bbcp: Copying 5 files and 0 links in 1 directory.
File Jacana_Raw_Reads/Jacana GBS Raw Reads/.DS_Store created; 6148 bytes at 255.8 KB/s
File Jacana_Raw_Reads/Jacana GBS Raw Reads/C6RL0ANXX_1_fastq created; 67982686805 bytes at 76.8 MB/s
File Jacana_Raw_Reads/Jacana GBS Raw Reads/C6RL0ANXX_1_fastq.gz created; 18058122029 bytes at 67.9 MB/s
File Jacana_Raw_Reads/Jacana GBS Raw Reads/C6RL0ANXX_2_fastq.gz created; 17324891025 bytes at 67.2 MB/s
File Jacana_Raw_Reads/Jacana GBS Raw Reads/C6RL0ANXX_3_fastq.gz created; 17453590465 bytes at 64.3 MB/s
5 files copied at effectively 71.7 MB/s\
```

* I'm not sure how long mine took, but at least half an hour I think. Now you are ready to use cypress to dig through this dataset!

Now, we're ready to run pyrad. 

####Running pyrad with a full dataset

####*Step 1*

* First, we need to create a new params file. You will also need the proper barcode files from Sara copied to your working folder on cypress. You can use scp to do this if you get them on your home computer (see example of using scp to copy to cypress above).

```bash
module load pyrad
pyrad -n
```

* You should see a new file, params.txt. We need to edit this params file to read our fastq file for plate 1 and provide it with a barcodes file. In our directory, there are three fastq files that correspond to the three plates of DNA that Sara ran. The barcodes overlap in these three plates, so we will have to run step 1 (demultiplexing) three times. The change to line two reflects this, where we only want it to consider the first plate. Line three specifies the one barcode file we are using (jacana1 corresponds to plate 1). 

* Here is what I have in my params file by copying the commands used in the pyRAD GBS tutorial.

```bash
==** parameter inputs for pyRAD version 3.0.66  **======================== affected step ==
./                        ## 1. Working directory                                 (all)
./C6RL0ANXX_1.fastq.gz    ## 2. Loc. of non-demultiplexed files (if not line 18)  (s1)
./jacana1.barcodes              ## 3. Loc. of barcode file (if not line 18)             (s1)
vsearch                   ## 4. command (or path) to call vsearch (or usearch)    (s3,s6)
muscle                    ## 5. command (or path) to call muscle                  (s3,s7)
TGCAG                     ## 6. Restriction overhang (e.g., C|TGCAG -> TGCAG)     (s1,s2)
20                         ## 7. N processors (parallel)                           (all)
6                         ## 8. Mindepth: min coverage for a cluster              (s4,s5)
4                         ## 9. NQual: max # sites with qual < 20 (or see line 20)(s2)
.85                 ## 10. lowered clust thresh...
gbs                 ## 11. changed datatype to gbs
4                         ## 12. MinCov: min samples in a final locus             (s7)
3                         ## 13. MaxSH: max inds with shared hetero site          (s7)
c85m4p3             ## 14. outprefix...
==== optional params below this line ===================================  affected step ==
                       ## 15.opt.: select subset (prefix* only selector)            (s2-s7)
                       ## 16.opt.: add-on (outgroup) taxa (list or prefix*)         (s6,s7)
                       ## 17.opt.: exclude taxa (list or prefix*)                   (s7)
                       ## 18.opt.: loc. of de-multiplexed data                      (s2)
                       ## 19.opt.: maxM: N mismatches in barcodes (def= 1)          (s1)
                       ## 20.opt.: phred Qscore offset (def= 33)                    (s2)
1                    ## 21. set filter to 1
                       ## 22.opt.: a priori E,H (def= 0.001,0.01, if not estimated) (s5)
                       ## 23.opt.: maxN: max Ns in a cons seq (def=5)               (s5)
8                    ## 24. increased maxH
                       ## 25.opt.: ploidy: max alleles in cons seq (def=2;see docs) (s4,s5)
                       ## 26.opt.: maxSNPs: (def=100). Paired (def=100,100)         (s7)
                       ## 27.opt.: maxIndels: within-clust,across-clust (def. 3,99) (s3,s7)
                       ## 28.opt.: random number seed (def. 112233)              (s3,s6,s7)
                       ## 29.opt.: trim overhang left,right on final loci, def(0,0) (s7)
*                    ## 30. all output formats
                       ## 31.opt.: maj. base call at depth>x<mindepth (def.x=mindepth) (s5)
50                   ## 32. keep fragments longer than 50
                       ## 33.opt.: max stack size (int), def= max(500,mean+2*SD)    (s3)
                       ## 34.opt.: minDerep: exclude dereps with <= N copies, def=1 (s3)
                       ## 35.opt.: use hierarchical clustering (def.=0, 1=yes)      (s6)
                       ## 36.opt.: repeat masking (def.=1='dust' method, 0=no)      (s3,s6)
                       ## 37.opt.: vsearch max threads per job (def.=6; see docs)   (s3,s6)
==== optional: list group/clade assignments below this line (see docs) ==================
```

* During the workshop, there may have been a slightly different params file used, I am not sure. But I was able to use the one above to work and it follows the default GBS parameters listed on the pyRAD tutorial for GBS SE reads.  

####This is a really useful time to review which of these parameters could be adjusted at this step! You can figure this out be looking at which of these parameters affect which step.
   * Lines 2 and 3 of course are used here, because they specify the location of the files being used. 
   * Line 5 specifies the restriction overhang. This differs between different types of protocols used for digesting genomic DNA.
   * Line 19 is the most important to pay attention to. This sets a threshold for how many mismatches the software will allow between the barcode you give it and the sequences it finds in your data. Mismatches exist, because the sequencer can make errors leading to an incorrect base read or not calling any base at all. The default, set to 1, shouuld only include barcodes the algorithm finds with up to 1 base pair mis matches. You could increase this and see how your output changes. 

* In running step 1, we found an issue with the barcodes file, since it was copied from excel and still has tabs instead of spaces. Let's replace tabs and then rename the files with the following commands:

```bash
cat jacana1.barcodes | sed -e 's/['$'\011'' ]\+/ /' | tee jacana1new.barcodes
mv jacana1.barcodes jacana1old.barcodes
mv jacana1new.barcodes jacana1.barcodes
```

* You will need to do this for each of the three barcodes files.

* Now, because we are on the cluster, we need to create a job script to run step one. I've called my script pyrad_1.1.srun. We also modified this script so that we can run step 1 on each plates without overwriting the output stats and fastq folders: 

```bash
#!/bin/bash
#SBATCH --qos=normal
#SBATCH --time=1-0
#SBATCH --verbose    ###        Verbosity logs error information into the error file
#SBATCH --job-name=jacana_pyrad_s1.1 ### Job Name
#SBATCH --nodes=1             ### Node count required for the job
#SBATCH --ntasks-per-node=20   ### Number of tasks to be launched per Node
#SBATCH --output=jacanas.1.1.output.out
#SBATCH --error=jacanas.1.1.error.err
#SBATCH --mail-type=ALL
#SBATCH --mail-user=slipshut@tulane.edu

date
pwd

mkdir w$SLURM_JOBID
cp params1.1.txt w$SLURM_JOBID/params.txt
ln C6RL0ANXX_1_fastq.gz w$SLURM_JOBID
ln jacana1.barcodes w$SLURM_JOBID
cd w$SLURM_JOBID


module load pyrad

pyrad -p params.txt -s 1

date (END) 
```

Summary of what we wrote into this script below the general sbatch commands:

   * In short, we create a working directory labeled with the slurm job ticket ID
   * Next, we copy the params file for plate one into this new folder and call it params.txt
   * Create a hard link to the fastq file in that new folder. We aren't taking up extra space by copying the file itself. 
   * Same for the barcodes file
   * Using cd navigate into the 
   * Run pyrad commands

* Continue running step 1 for plates 2 and 3 by making new params.txt files (e.g. params1.1.txt, params1.2.txt) and running new .srun scripts to include this new files. So you should have three params files and three .srun scripts. The result will be three new directories for each job that you ran. Within these directories are the results of de-multiplexing performed in pyRAD step 1. 

* We were able to successfully run step 1 of pyrad using the abpve script and params.txt files. After this finished we explored the outputs.
```bash
#navigate into the working directory labeled with the slurm ID
ls fastq/ | wc –l 	#tells you how many files you have in this fastq folder
wc -l jacana1.barcodes 	#tells you how many lines are in this file (95 is ok, 96 is ok too – depends on how computer is counting lines)
```

* Once a job is submitted you will have a new folder for that job – you want to keep the right folder, rename it (using the mv command) and get rid of old ones. I renamed this using the "mv" command to have a short description of what params I used (e.g. w249134_GBS_Params_S1.3). Below are a couple tips for exploring the directories we made.   
```bash
ls w[folder number] 	#this should show all the inputs and outputs from this job including a stats and fastq

du --si w[folder number] 	#this is another way to see if it worked, it shows you what’s in the folder and how much space it takes up
```

* You can explore the successful job output folders and navigate into them to explore the fastq and stats outputs

* We saw that line 19 (maxM)in the params file sets levels of more or les stringency – the default is 1 (one mismatch), but you can change this (see above). You can explore how well this method performed by looking at pyRAD out put found in the stats folder generated in step 1. 

```bash
cat stats/s1.sorting.txt | column –t | head 	#This will show you the beginning of the file with number of reads, etc in columns
```

* Your output should look something like this. You can see there is a discrepency between how many reads were found and how many were matched. That difference is how many reads were discarded in this step due to low confidence regarding that read's barcode match. 

```bash
file                  Nreads      cut_found   bar_matched
C6RL0ANXX_1_fastq.gz  267132531   257022443   254647086
sample                true_bar    obs_bars    N_obs
SL_161                AACGCACATT  AACGCACATT  3064528
SL_161                AACGCACATT  AACGCGCATT  33632
SL_161                AACGCACATT  ATCGCACATT  27169
SL_161                AACGCACATT  AACGGACATT  25965
SL_161                AACGCACATT  AACGCACTTT  20967
SL_161                AACGCACATT  AACGAACATT  18723
SL_161                AACGCACATT  AACGCACATG  18034
```

####*Step 2*

* Alright so now that we have three directories that contain fastq folder containing all of our de-multiplexed reads, we have to move these all into one directory. There is almost certainly some easy automated way of doing this, but I went for brute force. 

* The first thing I did make a new folder called step_2. Then I copied in the params file into it and rename it just params.txt (because we will only need one from here on out). Also copy in one of the job script files and call it step 2. 

```bash
mkdir step_2
mkdir step_2/fastq
cp params1.1.txt step_2/params.txt
cp pyrad_1.1.srun step_2/pyrad_2.srun
```

* What I did next is probably not the best way to do it, but here was my workflow:

```bash
cp -Rv --backup=existing --suffix=_plate3 w249134_GBS_Params_S1.1/fastq/*.gz step_2/fastq/
cp -Rv --backup=existing --suffix=_plate3 w249135_GBS_Params_S1.2/fastq/*.gz step_2/fastq/
cp -Rv --backup=existing --suffix=_plate3 w249136_GBS_Params_S1.3/fastq/*.gz step_2/fastq/
```

* This copies everything that was in my output directory from running step 1 for plate one into the fastq directory in the step_2 folder.
   * -R is recursive, so it copies everythign within that directory. Notice how I used *.gz to copy all .gz files
   * v is verbose so it will tell me exactly what is copying
   * --backup=existing says to do something with existing files
   * --suffix adds on a custom string to the end of any files that are duplicates in the destination directory

* Now, this is a poor mans solution, becuase now I have several files that end in .fq.gz_plate3 and every file needs to end in .fq.gz. So Rather than try to solve this problem and do it different, I just used mv to change the name of those repeats. Examples below:

```bash
  mv blank_R1.fq.gz_plate3 blank_R1_plate3.fq.gz
  mv LSUMZ_25287_R1.fq.gz_plate3 LSUMZ_25287_R1_plate3.fq.gz 
  mv MJM_7661_R1.fq.gz_plate3 MJM_7661_R1_plate3.fq.gz
  mv MJM_8247_R1.fq.gz_plate3 MJM_8247_R1_plate3.fq.gz
  mv SEL_STRI_25_R1.fq.gz_plate3 SEL_STRI_25_R1_plate3.fq.gz
  mv SL_161_R1.fq.gz_plate3 SL_161_R1_plate3.fq.gz
```


* Yay! Now we have a fastq folder filled with all the de multiplexed files! I am sure there was a better way to do this...
* Next, lets make a new job script. While preparing this, I noted that I could not make a hard link to the fastq directory (just cant do this with directories). Rather than spending time trying to find an alternative, I just altered the params.txt slightly (using nano params.txt). On line 18 you can specify the location of the directory where all your demultiplexed files are. Here is what that line looks like for me. 

```bash
/lustre/project/jk/Jacana_pyRAD/step_2/fastq/  ## 18.opt.: loc. of de-multiplexed data                      (s2)
```
* You don't have to change anything else in the params file at this point. However, this is a great opportunity to see what there is that could be changed!
   * Line 15: You could only select a subset of your data, for some reason, and this would help you do that
   * Line 20: In this step will will be filtering low quality data based on the phred score. This is an important line where you specify how high you want to set this bar for filtering. Good place to go back through and run again to see how this changes your data!
   * Line 21: This gives you several options for filtering based on quality or quality and adapters. We have adapters (when wouldn't you?) so we want to filter by both NQUAL+adapters, so this should be set to 1. Mine was already set this way, but you may need to set it yourself. I am not sure what 2 would do, but I bet its in the general tutorial. 
   * Line 32: This allows you to set the minimum length of reads you want to keep after trimming has been done. The default in the GBS tutorial is 50, so you should set it as such. Another good place to tweek when exploring your data. 

*Next I did some small tweeking to the job script (nano pyrad_2.srun), which now looks like this:

```bash
#!/bin/bash
#SBATCH --qos=normal
#SBATCH --time=1-0
#SBATCH --verbose    ###        Verbosity logs error information into the error file
#SBATCH --job-name=jacana_pyrad_s2 ### Job Name
#SBATCH --nodes=1             ### Node count required for the job
#SBATCH --ntasks-per-node=20   ### Number of tasks to be launched per Node
#SBATCH --output=jacanas.2.output.out
#SBATCH --error=jacanas.2.error.err
#SBATCH --mail-type=ALL
#SBATCH --mail-user=eenbody@tulane.edu

date
pwd

mkdir w$SLURM_JOBID
cp params.txt w$SLURM_JOBID/params.txt
cd w$SLURM_JOBID

module load pyrad

pyrad -p params.txt -s 2

date
```

* Run step 2!

```bash
sbatch pyrad_2.srun
```

* Remember you can check the status of your job submission with
```bash
squeue -u eenbody
```
